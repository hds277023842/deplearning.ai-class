{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-04T01:43:53.141184Z",
     "start_time": "2020-11-04T01:43:50.777377Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-04T01:43:54.365030Z",
     "start_time": "2020-11-04T01:43:54.346019Z"
    }
   },
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    A = 1/(1+np.exp(-Z))\n",
    "    cache = Z\n",
    "    return A, cache\n",
    "def sigmoid_backward(dA, cache):\n",
    "    Z = cache\n",
    "    s = 1/(1+np.exp(-Z))\n",
    "    dZ = dA * s * (1-s)\n",
    "    assert (dZ.shape == Z.shape)\n",
    "    return dZ\n",
    "def relu(Z):\n",
    "    A = np.maximum(0,Z)\n",
    "    assert(A.shape == Z.shape)\n",
    "    cache = Z \n",
    "    return A, cache\n",
    "def relu_backward(dA, cache):\n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True) # just converting dz to a correct object.\n",
    "    # When z <= 0, you should set dz to 0 as well. \n",
    "    dZ[Z <= 0] = 0\n",
    "    assert (dZ.shape == Z.shape)\n",
    "    return dZ\n",
    "def load_dataset():\n",
    "    train_dataset = h5py.File('datasets/train_catvnoncat.h5', \"r\")\n",
    "    train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:]) # your train set features\n",
    "    train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:]) # your train set labels\n",
    "\n",
    "    test_dataset = h5py.File('datasets/test_catvnoncat.h5', \"r\")\n",
    "    test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:]) # your test set features\n",
    "    test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:]) # your test set labels\n",
    "\n",
    "    classes = np.array(test_dataset[\"list_classes\"][:]) # the list of classes\n",
    "    \n",
    "    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n",
    "    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n",
    "    \n",
    "    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-04T03:08:07.718042Z",
     "start_time": "2020-11-04T03:08:07.678518Z"
    }
   },
   "outputs": [],
   "source": [
    "# 初始化参数\n",
    "def initialize_parameters(n_x,n_h,n_y):\n",
    "    W1 = np.random.randn(n_h, n_x) * 0.01\n",
    "    b1 = np.zeros((n_h, 1))\n",
    "    W2 = np.random.randn(n_y, n_h) * 0.01\n",
    "    b2 = np.zeros((n_y, 1))\n",
    "    parameters = {\"w1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"w2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters  \n",
    "# 初始化参数——针对深层神经网络\n",
    "def initialize_parameters_deep(layers_dims):\n",
    "    np.random.seed(3)\n",
    "    parameters = {}\n",
    "    l = len(layers_dims)\n",
    "    \n",
    "    for i in range(1,l):\n",
    "        parameters['w'+str(i)] = np.random.randn(layers_dims[i],layers_dims[i-1]) / np.sqrt(layers_dims[i-1])\n",
    "        parameters['b'+str(i)] = np.zeros(shape = (layers_dims[i],1))\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-04T03:08:07.863643Z",
     "start_time": "2020-11-04T03:08:07.808604Z"
    }
   },
   "outputs": [],
   "source": [
    "# 前向传播\n",
    "# 计算z\n",
    "def linear_forward(A,w,b):\n",
    "    z = np.dot(w,A) + b\n",
    "    cache = (A,w,b)\n",
    "    return z,cache\n",
    "# 计算A\n",
    "def linear_activation_forward(A_pre,w,b,activation):\n",
    "    if activation == 'relu':\n",
    "        z,linear_cache = linear_forward(A_pre,w,b)\n",
    "        A,activation_cache = relu(z)\n",
    "    if activation == 'sigmoid':\n",
    "        z,linear_cache = linear_forward(A_pre,w,b)\n",
    "        A,activation_cache = sigmoid(z)\n",
    "    cache = (linear_cache,activation_cache)\n",
    "    return A,cache\n",
    "# 深层的模型前向传播\n",
    "def L_model_forward(X,parameters):\n",
    "    L = len(parameters) // 2\n",
    "    A = X\n",
    "    caches = []\n",
    "    for l in range(1,L):\n",
    "        A_pre = A\n",
    "        A,cache = linear_activation_forward(A_pre,parameters['w'+str(l)],parameters['b'+str(l)],'relu')\n",
    "        caches.append(cache)\n",
    "    A_L,cache = linear_activation_forward(A_pre,parameters['w'+str(l)],parameters['b'+str(l)],'sigmoid')\n",
    "    caches.append(cache)\n",
    "    return A_L,caches\n",
    "# 计算损失\n",
    "def compute_cost(A_L,y):\n",
    "    m = y.shape[1]\n",
    "    cost = -np.sum(np.multiply(np.log(A_L),y) + np.multiply(np.log(1 - A_L), 1 - y)) / m\n",
    "        \n",
    "    cost = np.squeeze(cost)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-04T03:08:08.000741Z",
     "start_time": "2020-11-04T03:08:07.973722Z"
    }
   },
   "outputs": [],
   "source": [
    "# 线性部分反向\n",
    "def linear_backward(dz,cache):\n",
    "    # output layer\n",
    "    A_pre,w,b = cache\n",
    "    m = A_pre.shape[1]\n",
    "    # dz为第l层的线性输出成本梯度\n",
    "    dw = (1/m) * np.dot(dz,A_pre.T)\n",
    "    db = (1/m) * np.sum(dz,axis = 1,keepdims = True)\n",
    "    dA_pre = np.dot(w.T,dz) \n",
    "    \n",
    "    return dA_pre,dw,db\n",
    "# 激活函数部分反向\n",
    "def linear_activation_backward(dA,cache,activation = 'relu'):\n",
    "    # cache记录了用于有效计算反向传播的元组\n",
    "    linear_cache,activation_cache = cache\n",
    "    if activation == 'relu':\n",
    "        dz = relu_backward(dA,activation_cache)\n",
    "        dA_pre,dw,db = linear_backward(dz,linear_cache)\n",
    "    if activation == 'sigmoid':\n",
    "        dz = sigmoid_backward(dA,activation_cache)\n",
    "        dA_pre,dw,db = linear_backward(dz,linear_cache)\n",
    "    return dA_pre,dw,db\n",
    "\n",
    "def L_model_backward(A_L,y,caches):\n",
    "    grads = {}\n",
    "    L = len(caches)\n",
    "    m = AL.shape[1]\n",
    "    dAL = -np.divide(y,A_L) + np.divide(1-y,1-A_L)\n",
    "    \n",
    "    current_cache = caches[L-1]\n",
    "    grads['dA'+str(L)],grads['dw' + str(L)],grads['db'+str(L)] = linear_activation_backward(dAL,current_cache,'sigmoid')\n",
    "    for l in reversed(range(L-1)):\n",
    "        current_cache = caches[l]\n",
    "        dA_temp,dw_temp,db_temp = linear_activation_backwarde(grads['dA'+ str(l+2)],current_cache)\n",
    "        grads['dA'+str(l+1)] = dA_temp\n",
    "        grads['dw'+str(l+1)] = dw_temp\n",
    "        grads['db'+str(l+1)] = db_temp\n",
    "    return grads\n",
    "# 更新参数\n",
    "def update_parameters(parameters,grads,learning_rate = 0.1):\n",
    "    L = len(parameters) // 2\n",
    "    # print(grads)\n",
    "    for i in range(L):\n",
    "        parameters['w'+str(i+1)] -= learning_rate * grads['dw'+str(i+1)]\n",
    "        parameters['b'+str(i+1)] -= learning_rate * grads['db'+str(i+1)]\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-04T03:08:08.153349Z",
     "start_time": "2020-11-04T03:08:08.126830Z"
    }
   },
   "outputs": [],
   "source": [
    "# build_model\n",
    "def two_layer_model(X,y,layers_dims,learning_rate = 0.0075,num_iterations = 1000,print_cost = False):\n",
    "    np.random.seed(1)\n",
    "    grads = {}\n",
    "    costs = []\n",
    "    input_layers,hidden_layers,output_layers = layers_dims\n",
    "    # init操作\n",
    "    parameters = initialize_parameters(input_layers,hidden_layers,output_layers)\n",
    "    \n",
    "    w1 = parameters['w1']\n",
    "    w2 = parameters['w2']\n",
    "    b1 = parameters['b1']\n",
    "    b2 = parameters['b2']\n",
    "    # print(w1.shape,w2.shape,b1.shape,b2.shape)\n",
    "    for step in range(num_iterations):\n",
    "        # forward\n",
    "        A1,cache1 = linear_activation_forward(X,w1,b1,'relu')\n",
    "        A2,cache2 = linear_activation_forward(A1,w2,b2,'sigmoid')\n",
    "        \n",
    "        # costs\n",
    "        # print(A1.shape)\n",
    "        # print(A2.shape)\n",
    "        cost = compute_cost(A2,y)\n",
    "        \n",
    "        # backward\n",
    "        \n",
    "        dA2 = -np.divide(y,A2)+np.divide(1-y,1-A2)\n",
    "        \n",
    "        dA1,dw2,db2 = linear_activation_backward(dA2,cache2,'sigmoid')\n",
    "        dA0,dw1,db1 = linear_activation_backward(dA1,cache1,'relu')\n",
    "        grads['dw1'] = dw1\n",
    "        grads['dw2'] = dw2\n",
    "        grads['db1'] = db1\n",
    "        grads['db2'] = db2\n",
    "        # print(parameters)\n",
    "        \n",
    "        parameters = update_parameters(parameters,grads,learning_rate = learning_rate)\n",
    "        w1 = parameters['w1']\n",
    "        w2 = parameters['w2']\n",
    "        b1 = parameters['b1']\n",
    "        b2 = parameters['b2']\n",
    "        # print(w1)\n",
    "        \n",
    "        if step % 100 == 0:\n",
    "            print(cost)\n",
    "            costs.append(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-04T03:08:08.402528Z",
     "start_time": "2020-11-04T03:08:08.297953Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(209, 64, 64, 3) (50, 64, 64, 3) (1, 209) (1, 50)\n",
      "(12288, 209) (12288, 50)\n"
     ]
    }
   ],
   "source": [
    "train_x,train_y,test_x,test_y,classes = load_dataset()\n",
    "print(train_x.shape,test_x.shape,train_y.shape,test_y.shape)\n",
    "train_x = train_x.reshape(train_x.shape[0],-1).T\n",
    "test_x = test_x.reshape(test_x.shape[0],-1).T\n",
    "print(train_x.shape,test_x.shape)\n",
    "train_x = train_x / 255\n",
    "test_x = test_x / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-04T03:08:45.256237Z",
     "start_time": "2020-11-04T03:08:08.905914Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6930497356599891\n",
      "0.6464320953428849\n",
      "0.6325140647912677\n",
      "0.6015024920354665\n",
      "0.5601966311605748\n",
      "0.515830477276473\n",
      "0.47549013139433266\n",
      "0.4339163151225749\n",
      "0.400797753620389\n",
      "0.35807050113237987\n"
     ]
    }
   ],
   "source": [
    "input_layers = train_x.shape[0]\n",
    "hidden_layers = 7\n",
    "output_layers = train_y.shape[0]\n",
    "two_layer_model(train_x,train_y,[input_layers,hidden_layers,output_layers])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
